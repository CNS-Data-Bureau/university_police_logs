{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-instrument",
   "metadata": {},
   "source": [
    "# template_webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-replacement",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "private-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Reqeusts\n",
    "import requests\n",
    "# Other tools\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import fnmatch\n",
    "import os\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-senate",
   "metadata": {},
   "source": [
    "## Functions Using Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weighted-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_get_item(url, item):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}\n",
    "    if item == \"html\":       \n",
    "        page = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        return(soup)\n",
    "    \n",
    "    elif item == \"pdf\":\n",
    "        page = requests.get(url, headers = headers, stream=True)\n",
    "        return(page)\n",
    "    else:\n",
    "        print(\"Valid Item Not Selected\")\n",
    "\n",
    "def download_pdfs(ls_pdf_urls, download_path, file_name):\n",
    "    counter = 0 \n",
    "    for pdf_url in ls_pdf_urls:\n",
    "        counter = counter +1\n",
    "        g = requests_get_item(pdf_url, \"pdf\")\n",
    "        with open(f'{download_path}{file_name}_{counter}.pdf', 'wb') as sav:\n",
    "            for chunk in g.iter_content(chunk_size=1000000):\n",
    "                sav.write(chunk)\n",
    "        print(f\"download number: {counter}\")\n",
    "                \n",
    "                \n",
    "def convert_pdf_to_csv(pdf_directory, csv_directory):\n",
    "    directory = fr'{pdf_directory}'\n",
    "    directory_output = fr'{csv_directory}'\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):        \n",
    "        if file.endswith(\".pdf\"):\n",
    "            count = count + 1 \n",
    "            print(f'{directory}{file}: Conversion {count}')\n",
    "            tabula.convert_into(f'{directory}{file}', f'{directory_output}{count}.csv', output_format=\"csv\", pages='all')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organizational-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_data(data_out, database_name, file_path):\n",
    "        #writing data to json\n",
    "        data_out = data_out\n",
    "        database_name = database_name\n",
    "        file = f'{file_path}/{database_name}'\n",
    "        with open(file, 'w') as outfile:\n",
    "            json.dump(data_out, outfile)\n",
    "            \n",
    "def open_json_data(file_path):    \n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "composite-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2001-03-04', '2001-07-02', '2001-10-30', '2002-02-27', '2002-06-27', '2002-10-25', '2003-02-22', '2003-06-22', '2003-10-20', '2004-02-17', '2004-06-16', '2004-10-14', '2005-02-11', '2005-06-11', '2005-10-09', '2006-02-06', '2006-06-06', '2006-10-04', '2007-02-01', '2007-06-01', '2007-09-29', '2008-01-27', '2008-05-26', '2008-09-23', '2009-01-21', '2009-05-21', '2009-09-18', '2010-01-16', '2010-05-16', '2010-09-13', '2011-01-11', '2011-05-11', '2011-09-08', '2012-01-06', '2012-05-05', '2012-09-02', '2012-12-31', '2013-04-30', '2013-08-28', '2013-12-26', '2014-04-25', '2014-08-23', '2014-12-21', '2015-04-20', '2015-08-18', '2015-12-16', '2016-04-14', '2016-08-12', '2016-12-10', '2017-04-09', '2017-08-07', '2017-12-05', '2018-04-04', '2018-08-02', '2018-11-30', '2019-03-30', '2019-07-28', '2019-11-25', '2020-03-24', '2020-07-22', '2020-11-19', '2021-03-19', '2021-07-17', '2021-11-28']\n"
     ]
    }
   ],
   "source": [
    "# date must be in following format: 2021-08-12\n",
    "def get_date(start_date, end_date):\n",
    "    date_list = []\n",
    "    # end_date as current data\n",
    "    if end_date == \"today\":\n",
    "        end_date = date.today()\n",
    "    \n",
    "    # add 60 days to start date, save that as first el in list\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    start_date = start_date + timedelta(days=60)\n",
    "    start_date = (start_date.date())\n",
    "    \n",
    "    date_list.append(str(start_date))\n",
    "    while (start_date + timedelta(days=60)) < end_date:\n",
    "        start_date = start_date + timedelta(days=120)\n",
    "        string_start_date = str(start_date)\n",
    "        \n",
    "        if (start_date + timedelta(days=60)) > end_date: \n",
    "            start_date = end_date\n",
    "        date_list.append(str(start_date))\n",
    "    \n",
    "        \n",
    "    return(date_list)\n",
    "        # while start + 60 is less than today, keep adding a new date that's +120 days\n",
    "        # when it's greater thant today's date, break out of the lop\n",
    "    \n",
    "    # next date will be 120 days after the date we just returned\n",
    "    # while loop intil we hit a date that's within 60 days of today's end_date\n",
    "    \n",
    "dates = get_date('2001-01-03', 'today')\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "patient-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:30<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "### generate urls -\n",
    "# url = https://www.salisbury.edu/_services/police/crimebeat?date=07%2F12%2F2017\n",
    "\n",
    "\n",
    "base_url = 'https://www.salisbury.edu/_services/police/crimebeat?date='\n",
    "results = []\n",
    "counter = 0\n",
    "counter2 = 0\n",
    "for date in tqdm(dates):\n",
    "    split_date = date.split('-')\n",
    "    \n",
    "    year = (split_date[0])\n",
    "    month = (split_date[1])\n",
    "    day = (split_date[2])     \n",
    "    url = (f'{base_url}{month}%2F{day}%2F{year}')         \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')         \n",
    "    result = soup.text\n",
    "    file = f'../data/handmade/salisbury/jsons_final/salisbury_{year}_{month}_{day}.json'\n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(result, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
