{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "emotional-intellectual",
   "metadata": {},
   "source": [
    "# template_webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-architecture",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "governmental-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Reqeusts\n",
    "import requests\n",
    "# Other tools\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import fnmatch\n",
    "import os\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-rescue",
   "metadata": {},
   "source": [
    "## Functions Using Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "legendary-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_get_item(url, item):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}\n",
    "    if item == \"html\":       \n",
    "        page = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        return(soup)\n",
    "    \n",
    "    elif item == \"pdf\":\n",
    "        page = requests.get(url, headers = headers, stream=True)\n",
    "        return(page)\n",
    "    else:\n",
    "        print(\"Valid Item Not Selected\")\n",
    "\n",
    "def download_pdfs(ls_pdf_urls, download_path, file_name):\n",
    "    counter = 0 \n",
    "    for pdf_url in ls_pdf_urls:\n",
    "        counter = counter +1\n",
    "        g = requests_get_item(pdf_url, \"pdf\")\n",
    "        with open(f'{download_path}{file_name}_{counter}.pdf', 'wb') as sav:\n",
    "            for chunk in g.iter_content(chunk_size=1000000):\n",
    "                sav.write(chunk)\n",
    "        print(f\"download number: {counter}\")\n",
    "                \n",
    "                \n",
    "def convert_pdf_to_csv(pdf_directory, csv_directory):\n",
    "    directory = fr'{pdf_directory}'\n",
    "    directory_output = fr'{csv_directory}'\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):        \n",
    "        if file.endswith(\".pdf\"):\n",
    "            count = count + 1 \n",
    "            print(f'{directory}{file}: Conversion {count}')\n",
    "            tabula.convert_into(f'{directory}{file}', f'{directory_output}{count}.csv', output_format=\"csv\", pages='all')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-lover",
   "metadata": {},
   "source": [
    "## Functions Using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "funded-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important paths\n",
    "executable_path = \"./aadit_chromedriver\"\n",
    "download_path = f\"..data/handmade/salsbury/pdfs/\"\n",
    "\n",
    "# Define Routes (usually in xpaths)\n",
    "\n",
    "def browser_instance(executable_path, headless, download_path):\n",
    "    \n",
    "    \n",
    "    chrome_options =  Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")   \n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": download_path})    \n",
    "    driver = webdriver.Chrome(executable_path=executable_path, options = chrome_options)\n",
    "    return driver  \n",
    "\n",
    "def signIn(driver, url, signin_field, username_field, password_field):  \n",
    "    driver.get(url)\n",
    "    sign_in = driver.find_element_by_xpath(signin_field)\n",
    "    sign_in.click()\n",
    "    email_field = driver.find_element_by_xpath(username_field)\n",
    "    password_field = driver.find_element_by_xpath(password_field)\n",
    "    email_field.send_keys(USERNAME)\n",
    "    password_field.send_keys(PASSWORD + Keys.RETURN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-reason",
   "metadata": {},
   "source": [
    "## Useful Selenium Concepts\n",
    "### Waiting until element is clickable\n",
    "button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, download_all_field)))\n",
    "button.click()\n",
    "\n",
    "### Try and Expect\n",
    "try:\n",
    "    attempt to do something\n",
    "except:\n",
    "    do this if the try doesn't work out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-tuning",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "spatial-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_data(data_out, database_name, file_path):\n",
    "        #writing data to json\n",
    "        data_out = data_out\n",
    "        database_name = database_name\n",
    "        file = f'{file_path}/{database_name}'\n",
    "        with open(file, 'w') as outfile:\n",
    "            json.dump(data_out, outfile)\n",
    "            \n",
    "def open_json_data(file_path):    \n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "tired-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-03-04', '2019-07-02', '2019-10-30', '2020-02-27', '2020-06-26', '2020-10-24', '2021-02-21', '2021-06-21', '2021-10-12']\n"
     ]
    }
   ],
   "source": [
    "# date must be in following format: 2021-08-12\n",
    "def get_date(start_date, end_date):\n",
    "    date_list = []\n",
    "    # end_date as current data\n",
    "    end_date = date.today()\n",
    "    # add 60 days to start date, save that as first el in list\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    start_date = start_date + timedelta(days=60)\n",
    "    start_date = (start_date.date())\n",
    "    \n",
    "    date_list.append(str(start_date))\n",
    "    while (start_date + timedelta(days=60)) < end_date:\n",
    "        start_date = start_date + timedelta(days=120)\n",
    "        string_start_date = str(start_date)\n",
    "        \n",
    "        if (start_date + timedelta(days=60)) > end_date: \n",
    "            start_date = end_date\n",
    "        date_list.append(str(start_date))\n",
    "    \n",
    "        \n",
    "    return(date_list)\n",
    "        # while start + 60 is less than today, keep adding a new date that's +120 days\n",
    "        # when it's greater thant today's date, break out of the lop\n",
    "    \n",
    "    # next date will be 120 days after the date we just returned\n",
    "    # while loop intil we hit a date that's within 60 days of today's end_date\n",
    "    \n",
    "dates = get_date('2019-01-03', 'end_date')\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "blocked-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = browser_instance(executable_path, False, download_path)\n",
    "driver.get(\"https://www.salisbury.edu/police/clery-compliance/daily-crime-and-incident-log.aspx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = []\n",
    "for date in dates[0:1]:\n",
    "#     print(date)\n",
    "    driver.get(\"https://www.salisbury.edu/police/clery-compliance/daily-crime-and-incident-log.aspx\")\n",
    "    search_bar = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dateSearch\"]')))\n",
    "    submit_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"dateSubmit\"]')))\n",
    "    \n",
    "    print(date)\n",
    "    print(\"-----\")\n",
    "    search_bar.send_keys(date)\n",
    "    \n",
    "    submit_button.click()\n",
    "#     search_bar.send_keys(Keys.DELETE)\n",
    "#     search_bar.send_keys(Keys.COMMAND, 'a')\n",
    "#     search_bar.send_keys(Keys.BACKSPACE)\n",
    "    table_finder = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"crimebeattable\"]')))\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    print(soup.prettify())\n",
    "#     table = soup.find(id=\"crimebeattable\")\n",
    "#     rows = table.find_all('tr')\n",
    "#     for row in rows[1:]:\n",
    "#       cells = row.find_all('td')\n",
    "#       results.append([cell.text for cell in cells])\n",
    "        \n",
    "# with open('1.csv','w') as output_file:\n",
    "#     csvfile = csv.writer(output_file)\n",
    "#     csvfile.writerow(['case', 'date_time_location','date_reported', 'incident', 'disposition'])\n",
    "#     csvfile.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "industrial-smooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "### generate urls -\n",
    "# url = https://www.salisbury.edu/_services/police/crimebeat?date=07%2F12%2F2017\n",
    "\n",
    "\n",
    "base_url = 'https://www.salisbury.edu/_services/police/crimebeat?date='\n",
    "results = []\n",
    "counter = 0\n",
    "counter2 = 0\n",
    "for date in dates:\n",
    "#     print(date)\n",
    "    pass\n",
    "    split_date = date.split('-')\n",
    "    for num in split_date:\n",
    "        year = (split_date[0])\n",
    "        month = (split_date[1])\n",
    "        day = (split_date[2])\n",
    "#         print(year, month, year)\n",
    "#         print('---')\n",
    "#         print(url)\n",
    "#         print('---')\n",
    "        url = (f'{base_url}{month}%2F{day}%2F{year}')\n",
    "#         print(url)\n",
    "#         counter2 = counter2 + 1\n",
    "#         print(counter2)\n",
    "#         print('---')\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#         counter2 = counter2 + 1\n",
    "#         print(counter2)\n",
    "        results.append(soup.text)\n",
    "#         print(results)\n",
    "for result in results:\n",
    "    \n",
    "    counter = counter + 1\n",
    "    print(counter)\n",
    "#             print(type(result))\n",
    "#             print('-----------')\n",
    "    file = f'../data/handmade/salisbury/jsons/{counter}output.json'\n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(result, outfile)\n",
    "            \n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_json_data(data_out, database_name, file_path):\n",
    "        #writing data to json\n",
    "        data_out = data_out\n",
    "        database_name = database_name\n",
    "        file = f'{file_path}/{database_name}'\n",
    "        with open(file, 'w') as outfile:\n",
    "            json.dump(data_out, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-german",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-encounter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
