{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "romance-wayne",
   "metadata": {},
   "source": [
    "# salsbury"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-liverpool",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "specified-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Reqeusts\n",
    "import requests\n",
    "# Other tools\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    " \n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import fnmatch\n",
    "import os\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "empirical-worse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-14\n",
      "2020-12-14\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.strptime(\"2020-12-12\", '%Y-%m-%d')\n",
    "start_date = start_date + timedelta(days=2)\n",
    "\n",
    "start_date = (start_date.date())\n",
    "print(start_date)\n",
    "# start_date  < date.today()\n",
    "print(start_date)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "organic-arlington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-07-02',\n",
       " '2017-10-30',\n",
       " '2018-02-27',\n",
       " '2018-06-27',\n",
       " '2018-10-25',\n",
       " '2019-02-22',\n",
       " '2019-06-22',\n",
       " '2019-10-20',\n",
       " '2020-02-17',\n",
       " '2020-06-16',\n",
       " '2020-10-14',\n",
       " '2021-02-11',\n",
       " '2021-06-11',\n",
       " '2021-10-07']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get to the page, insert each element from our list to our searchbox and then download the html that it returns \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-shark",
   "metadata": {},
   "source": [
    "## Functions Using Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simplified-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_get_item(url, item):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}\n",
    "    if item == \"html\":       \n",
    "        page = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        return(soup)\n",
    "    \n",
    "    elif item == \"pdf\":\n",
    "        page = requests.get(url, headers = headers, stream=True)\n",
    "        return(page)\n",
    "    else:\n",
    "        print(\"Valid Item Not Selected\")\n",
    "\n",
    "def download_pdfs(ls_pdf_urls, download_path, file_name):\n",
    "    counter = 0 \n",
    "    for pdf_url in ls_pdf_urls:\n",
    "        counter = counter +1\n",
    "        g = requests_get_item(pdf_url, \"pdf\")\n",
    "        with open(f'{download_path}{file_name}_{counter}.pdf', 'wb') as sav:\n",
    "            for chunk in g.iter_content(chunk_size=1000000):\n",
    "                sav.write(chunk)\n",
    "        print(f\"download number: {counter}\")\n",
    "                \n",
    "                \n",
    "def convert_pdf_to_csv(pdf_directory, csv_directory):\n",
    "    directory = fr'{pdf_directory}'\n",
    "    directory_output = fr'{csv_directory}'\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):        \n",
    "        if file.endswith(\".pdf\"):\n",
    "            count = count + 1 \n",
    "            print(f'{directory}{file}: Conversion {count}')\n",
    "            tabula.convert_into(f'{directory}{file}', f'{directory_output}{count}.csv', output_format=\"csv\", pages='all')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-moral",
   "metadata": {},
   "source": [
    "## Functions Using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important paths\n",
    "executable_path = \"C:/Users/nicho/Documents/Code/university_police_logs/webscrapers/chromedriver.exe\"\n",
    "download_path = f\"C:\\\\Users\\\\nicho\\\\Documents\\\\Code\\\\university_police_logs\\\\data\\\\handmade\\\\gt\\\\pdfs\\\\\"\n",
    "\n",
    "# Define Routes (usually in xpaths)\n",
    "\n",
    "def browser_instance(executable_path, headless, download_path):\n",
    "    \n",
    "    \n",
    "    chrome_options =  Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")   \n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": download_path})    \n",
    "    driver = webdriver.Chrome(executable_path=executable_path, options = chrome_options)\n",
    "    return driver  \n",
    "\n",
    "def signIn(driver, url, signin_field, username_field, password_field):  \n",
    "    driver.get(url)\n",
    "    sign_in = driver.find_element_by_xpath(signin_field)\n",
    "    sign_in.click()\n",
    "    email_field = driver.find_element_by_xpath(username_field)\n",
    "    password_field = driver.find_element_by_xpath(password_field)\n",
    "    email_field.send_keys(USERNAME)\n",
    "    password_field.send_keys(PASSWORD + Keys.RETURN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-photography",
   "metadata": {},
   "source": [
    "## Useful Selenium Concepts\n",
    "### Waiting until element is clickable\n",
    "button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, download_all_field)))\n",
    "button.click()\n",
    "\n",
    "### Try and Expect\n",
    "try:\n",
    "    attempt to do something\n",
    "except:\n",
    "    do this if the try doesn't work out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-cherry",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_data(data_out, database_name, file_path):\n",
    "        #writing data to json\n",
    "        data_out = data_out\n",
    "        database_name = database_name\n",
    "        file = f'{file_path}/{database_name}'\n",
    "        with open(file, 'w') as outfile:\n",
    "            json.dump(data_out, outfile)\n",
    "            \n",
    "def open_json_data(file_path):    \n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-citizenship",
   "metadata": {},
   "source": [
    "## Get 'em links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "parallel-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_files/pdfs/logs/2021-crime-log.pdf\n",
      "_files/pdfs/logs/2011-20xx-police-crime-log.pdf\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#script\n",
    "\n",
    "page = requests_get_item('https://www.frostburg.edu/about-frostburg/Administrative-Offices/university-police/police-crime-log.php' , 'html')\n",
    "div = page.find('div', class_=\"main-content\")\n",
    "# print(div)\n",
    "link_list = []\n",
    "final_link_list = []\n",
    "base_url = 'https://www.frostburg.edu/about-frostburg/Administrative-Offices/university-police/'\n",
    "\n",
    "links = div.find_all('a')\n",
    "\n",
    "for link in links:\n",
    "    if 'href' in str(link):\n",
    "        link = (link[\"href\"])\n",
    "        link_list.append(link)\n",
    "        print(link)\n",
    "print(final_link_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-market",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "grateful-merit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.frostburg.edu/about-frostburg/Administrative-Offices/university-police/_files/pdfs/logs/2021-crime-log.pdf']\n",
      "---\n",
      "['https://www.frostburg.edu/about-frostburg/Administrative-Offices/university-police/_files/pdfs/logs/2021-crime-log.pdf', 'https://www.frostburg.edu/about-frostburg/Administrative-Offices/university-police/_files/pdfs/logs/2011-20xx-police-crime-log.pdf']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_link_list = []\n",
    "base_url = 'https://www.frostburg.edu/about-frostburg/Administrative-Offices/university-police/'\n",
    "for link in link_list:\n",
    "#     print(link)\n",
    "#     print('---')\n",
    "    if link.startswith(\"_files\"):\n",
    "        final_link = base_url + link\n",
    "        final_link_list.append(final_link)\n",
    "        print(final_link_list)\n",
    "        print('---')\n",
    "    else:\n",
    "        print('check url format')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-religion",
   "metadata": {},
   "source": [
    "## Download 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "major-cocktail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download number: 1\n",
      "download number: 2\n"
     ]
    }
   ],
   "source": [
    "download_path = \"../data/handmade/frostburg/pdfs/\"\n",
    "download_pdfs(final_link_list, download_path, \"towson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-elevation",
   "metadata": {},
   "source": [
    "## Convert 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "alone-agreement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/handmade/frostburg/pdfs/towson_2.pdf: Conversion 1\n",
      "../data/handmade/frostburg/pdfs/towson_1.pdf: Conversion 2\n"
     ]
    }
   ],
   "source": [
    "csv_directory = \"../data/handmade/frostburg/csvs/\"\n",
    "convert_pdf_to_csv(download_path, csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-breast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
