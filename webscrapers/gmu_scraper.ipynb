{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a11e03a-333f-42cc-9205-1a717373405c",
   "metadata": {},
   "source": [
    "# GMU Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca4d68-b6b3-42a9-8043-a59d8bc1452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854164f6-94e6-48d8-a32d-7c9af95cf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "# Reqeusts\n",
    "import requests\n",
    "# Other tools\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import fnmatch\n",
    "import os\n",
    "import tabula\n",
    "from tabula.io import read_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e298d-52fd-4168-ad4e-9a6d66537947",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be06bc7-35a5-4747-a03e-68ae1558b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_get_item(url, item):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}\n",
    "    if item == \"html\":       \n",
    "        page = requests.get(url, headers = headers)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        return(soup)\n",
    "    \n",
    "    elif item == \"pdf\":\n",
    "        page = requests.get(url, headers = headers, stream=True)\n",
    "        return(page)\n",
    "    else:\n",
    "        print(\"Valid Item Not Selected\")\n",
    "\n",
    "def download_pdfs(ls_pdf_urls, download_path, file_name):\n",
    "    counter = 0 \n",
    "    for pdf_url in ls_pdf_urls:\n",
    "        counter = counter +1\n",
    "        g = requests_get_item(pdf_url, \"pdf\")\n",
    "        with open(f'{download_path}{file_name}_{counter}.pdf', 'wb') as sav:\n",
    "            for chunk in g.iter_content(chunk_size=1000000):\n",
    "                sav.write(chunk)\n",
    "        print(f\"download number: {counter}\")\n",
    "                \n",
    "                \n",
    "def convert_pdf_to_csv(pdf_directory, csv_directory):\n",
    "    directory = fr'{pdf_directory}'\n",
    "    directory_output = fr'{csv_directory}'\n",
    "    count = 0\n",
    "    for file in os.listdir(directory):\n",
    "        print(f'{directory}{file}')\n",
    "        if file.endswith(\".pdf\"):\n",
    "            count = count + 1 \n",
    "            tabula.convert_into(f'{directory}{file}', f'{directory_output}{count}.csv', output_format=\"csv\", pages='all')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081533fc-c180-4739-835f-6f23ea2cc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1e67d-c7d8-4aef-a581-23e0695a7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://police.gmu.edu/central-records/crime-log-test/\"\n",
    "\n",
    "def scrape_gmu(url):\n",
    "    # Load html\n",
    "    soup = requests_get_item(url, \"html\")\n",
    "    \n",
    "    # Isolate part of page where links are\n",
    "    middle_page = soup.find(\"div\", {\"id\": \"content-right\"})\n",
    "    urls = middle_page.find_all(\"a\")\n",
    "    \n",
    "    # Append only .pdf links to array\n",
    "    ls_pdf_links = []\n",
    "    for url in urls:\n",
    "        if (\".pdf\" in url.get('href', [])):\n",
    "            ls_pdf_links.append(url[\"href\"])\n",
    "        \n",
    "    # Download pdfs from array\n",
    "    download_pdfs(ls_pdf_links, \"../data/handmade/gmu/pdfs/\", \"gmu_scraped\")\n",
    "    \n",
    "    # Convert downloaded pdfs to csvs\n",
    "    convert_pdf_to_csv(\"../data/handmade/gmu/pdfs/\", \"../data/handmade/gmu/csvs/\")\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d52f0-4929-4de6-a69f-a57ab4503af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_gmu(url)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
